{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAJJY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-07-14 12:26:38,808] A new study created in memory with name: no-name-81119d68-c666-4378-ba58-7022afafe39a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Job Id  Burst time  Arrival Time  Prremptive  Resources\n",
      "0     247         199        0.4100           0          8\n",
      "1      29         193        0.5925           1          2\n",
      "2     170          75        0.3600           1          4\n",
      "3     164          42        0.9725           0          8\n",
      "4     312         257        0.6125           0          4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAJJY\\AppData\\Local\\Temp\\ipykernel_20464\\2538714430.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "C:\\Users\\RAJJY\\AppData\\Local\\Temp\\ipykernel_20464\\2538714430.py:58: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
      "C:\\Users\\RAJJY\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "[I 2024-07-14 12:27:36,465] Trial 0 finished with value: -92664.4 and parameters: {'learning_rate': 0.008739813379879766, 'batch_size': 128, 'gamma': 0.9202638901137492, 'n_steps': 8192}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:28:02,329] Trial 1 finished with value: -101716.0 and parameters: {'learning_rate': 0.00036198599807089676, 'batch_size': 256, 'gamma': 0.9244548302168227, 'n_steps': 4096}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:28:34,074] Trial 2 finished with value: -97240.8 and parameters: {'learning_rate': 0.0038330730701084175, 'batch_size': 128, 'gamma': 0.989765221196286, 'n_steps': 4096}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:29:19,583] Trial 3 finished with value: -101841.6 and parameters: {'learning_rate': 0.00017056381666650668, 'batch_size': 64, 'gamma': 0.9857073855695322, 'n_steps': 4096}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:30:19,858] Trial 4 finished with value: -103567.2 and parameters: {'learning_rate': 3.4703411071679635e-05, 'batch_size': 128, 'gamma': 0.9075365494904342, 'n_steps': 8192}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:31:44,976] Trial 5 finished with value: -93621.7 and parameters: {'learning_rate': 0.0018636550770564852, 'batch_size': 64, 'gamma': 0.9293220227705273, 'n_steps': 8192}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:32:34,978] Trial 6 finished with value: -100439.5 and parameters: {'learning_rate': 0.00044107899342713024, 'batch_size': 256, 'gamma': 0.9155293944187123, 'n_steps': 8192}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:33:02,300] Trial 7 finished with value: -101608.8 and parameters: {'learning_rate': 0.00011829254908264902, 'batch_size': 256, 'gamma': 0.9141260181771315, 'n_steps': 4096}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:33:47,368] Trial 8 finished with value: -102060.4 and parameters: {'learning_rate': 1.5090262991671354e-05, 'batch_size': 64, 'gamma': 0.9615507860854975, 'n_steps': 4096}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:34:21,376] Trial 9 finished with value: -105059.9 and parameters: {'learning_rate': 1.7609040940642634e-05, 'batch_size': 128, 'gamma': 0.9676364113853304, 'n_steps': 4096}. Best is trial 0 with value: -92664.4.\n",
      "[I 2024-07-14 12:35:28,142] Trial 10 finished with value: -14213.7 and parameters: {'learning_rate': 0.009090908217104239, 'batch_size': 32, 'gamma': 0.9394634895402538, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:36:35,451] Trial 11 finished with value: -162000.0 and parameters: {'learning_rate': 0.009440127316550221, 'batch_size': 32, 'gamma': 0.9417982676741307, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:37:42,935] Trial 12 finished with value: -85373.2 and parameters: {'learning_rate': 0.0016502257211651104, 'batch_size': 32, 'gamma': 0.9474130154646984, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:38:50,219] Trial 13 finished with value: -86236.2 and parameters: {'learning_rate': 0.0015791944423741032, 'batch_size': 32, 'gamma': 0.9478094484577817, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:39:57,019] Trial 14 finished with value: -86493.9 and parameters: {'learning_rate': 0.0012326111652278414, 'batch_size': 32, 'gamma': 0.9383454381661166, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:41:04,806] Trial 15 finished with value: -85221.1 and parameters: {'learning_rate': 0.0037023570461615155, 'batch_size': 32, 'gamma': 0.9614659441919895, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:42:12,623] Trial 16 finished with value: -88396.9 and parameters: {'learning_rate': 0.006726614960338627, 'batch_size': 32, 'gamma': 0.965037357799117, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:43:21,283] Trial 17 finished with value: -88869.2 and parameters: {'learning_rate': 0.0033192392563067942, 'batch_size': 32, 'gamma': 0.9785407036488684, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:44:28,884] Trial 18 finished with value: -90292.4 and parameters: {'learning_rate': 0.0007982709240149028, 'batch_size': 32, 'gamma': 0.9578889610174557, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:45:38,660] Trial 19 finished with value: -89122.7 and parameters: {'learning_rate': 0.004029258161458814, 'batch_size': 32, 'gamma': 0.9757682553515884, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:46:48,274] Trial 20 finished with value: -98716.4 and parameters: {'learning_rate': 0.0008043143887820721, 'batch_size': 32, 'gamma': 0.9978206087305765, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:47:58,761] Trial 21 finished with value: -86454.1 and parameters: {'learning_rate': 0.0022654789871960697, 'batch_size': 32, 'gamma': 0.9526491596909349, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:49:12,201] Trial 22 finished with value: -83739.4 and parameters: {'learning_rate': 0.005007392191769407, 'batch_size': 32, 'gamma': 0.9344518477289371, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:50:21,728] Trial 23 finished with value: -83496.1 and parameters: {'learning_rate': 0.005060188465616218, 'batch_size': 32, 'gamma': 0.9384336541953981, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:51:29,396] Trial 24 finished with value: -82905.6 and parameters: {'learning_rate': 0.005576432950331421, 'batch_size': 32, 'gamma': 0.9339674517833768, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:52:39,252] Trial 25 finished with value: -106500.0 and parameters: {'learning_rate': 0.00972434785714562, 'batch_size': 32, 'gamma': 0.9302933668712181, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:53:48,252] Trial 26 finished with value: -84291.7 and parameters: {'learning_rate': 0.002639190931923513, 'batch_size': 32, 'gamma': 0.9416329245105122, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:54:17,076] Trial 27 finished with value: -84316.2 and parameters: {'learning_rate': 0.0061096498821546115, 'batch_size': 256, 'gamma': 0.9044716808801291, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:55:05,819] Trial 28 finished with value: -90351.0 and parameters: {'learning_rate': 0.0008302223899927629, 'batch_size': 64, 'gamma': 0.9247039735037119, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:56:09,762] Trial 29 finished with value: -93128.1 and parameters: {'learning_rate': 0.005988905368996109, 'batch_size': 128, 'gamma': 0.9528765121299971, 'n_steps': 8192}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:57:18,550] Trial 30 finished with value: -102138.1 and parameters: {'learning_rate': 5.4941887099157635e-05, 'batch_size': 32, 'gamma': 0.9195394882931186, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:58:26,063] Trial 31 finished with value: -83717.9 and parameters: {'learning_rate': 0.005081401041423938, 'batch_size': 32, 'gamma': 0.9352678708139434, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 12:59:34,102] Trial 32 finished with value: -22128.2 and parameters: {'learning_rate': 0.009928114078285179, 'batch_size': 32, 'gamma': 0.9358767315056851, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:00:42,352] Trial 33 finished with value: -139879.7 and parameters: {'learning_rate': 0.008453735346916956, 'batch_size': 32, 'gamma': 0.9442505285455801, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:01:50,182] Trial 34 finished with value: -84410.3 and parameters: {'learning_rate': 0.0028960371482904036, 'batch_size': 32, 'gamma': 0.9254245999488206, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:02:42,407] Trial 35 finished with value: -92751.1 and parameters: {'learning_rate': 0.00931679342525591, 'batch_size': 256, 'gamma': 0.9319100749117075, 'n_steps': 8192}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:03:16,771] Trial 36 finished with value: -93393.4 and parameters: {'learning_rate': 0.004560345577045176, 'batch_size': 128, 'gamma': 0.9366659226568379, 'n_steps': 4096}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:04:03,100] Trial 37 finished with value: -97494.7 and parameters: {'learning_rate': 0.0003710853053902636, 'batch_size': 64, 'gamma': 0.9134617294923274, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:06:16,312] Trial 38 finished with value: -92874.1 and parameters: {'learning_rate': 0.00021886704849204217, 'batch_size': 32, 'gamma': 0.9211548224737459, 'n_steps': 8192}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:06:45,045] Trial 39 finished with value: -87262.7 and parameters: {'learning_rate': 0.006513022583074898, 'batch_size': 256, 'gamma': 0.9296392205049768, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:07:20,409] Trial 40 finished with value: -101268.9 and parameters: {'learning_rate': 0.0005623608623327624, 'batch_size': 128, 'gamma': 0.9527721980167833, 'n_steps': 4096}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:08:29,358] Trial 41 finished with value: -84498.1 and parameters: {'learning_rate': 0.004975630724643997, 'batch_size': 32, 'gamma': 0.938186370255516, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:09:38,038] Trial 42 finished with value: -86518.1 and parameters: {'learning_rate': 0.0020748699286290283, 'batch_size': 32, 'gamma': 0.9432695604206429, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:10:48,247] Trial 43 finished with value: -107979.1 and parameters: {'learning_rate': 0.007521233553743797, 'batch_size': 32, 'gamma': 0.9340680127684324, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:11:56,410] Trial 44 finished with value: -84795.4 and parameters: {'learning_rate': 0.0034571617396020517, 'batch_size': 32, 'gamma': 0.9262127308218618, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:13:29,825] Trial 45 finished with value: -94635.7 and parameters: {'learning_rate': 0.0013194001083029322, 'batch_size': 64, 'gamma': 0.9469162203703855, 'n_steps': 8192}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:14:40,135] Trial 46 finished with value: -100510.2 and parameters: {'learning_rate': 9.057010823321552e-05, 'batch_size': 32, 'gamma': 0.938946052435925, 'n_steps': 4096}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:15:51,629] Trial 47 finished with value: -73582.0 and parameters: {'learning_rate': 0.0050346702927844, 'batch_size': 32, 'gamma': 0.9191926554699679, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:17:03,048] Trial 48 finished with value: -55036.8 and parameters: {'learning_rate': 0.007420562895451728, 'batch_size': 32, 'gamma': 0.9167546872269897, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n",
      "[I 2024-07-14 13:18:14,027] Trial 49 finished with value: -44140.0 and parameters: {'learning_rate': 0.007621685738926148, 'batch_size': 32, 'gamma': 0.9096887932903305, 'n_steps': 2048}. Best is trial 10 with value: -14213.7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.009090908217104239, 'batch_size': 32, 'gamma': 0.9394634895402538, 'n_steps': 2048}\n",
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -1.03e+05 |\n",
      "| time/              |           |\n",
      "|    fps             | 1071      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 8192      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | -9.9e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024353597 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | -5.36e-06   |\n",
      "|    learning_rate        | 0.00909     |\n",
      "|    loss                 | 1.22e+05    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 1.52e+06    |\n",
      "-----------------------------------------\n",
      "total_rewards: -24707.1\n",
      "average_burst_time: 49.41420000000001\n",
      "average_waiting_time: 12480.573666499995\n",
      "average_turnaround_time: 12529.987866499996\n",
      "resource_utilization: 2.0064\n",
      "throughput: 0.020237211669307913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0., 199.,   8.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import optuna\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "process_data_path = r\"C:\\Users\\RAJJY\\Downloads\\process_data.csv\"\n",
    "df = pd.read_csv(process_data_path)\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Define the Custom CPU Scheduling Environment\n",
    "class SarahcustomRLEnv(gym.Env):\n",
    "    def __init__(self, process_data):\n",
    "        super(SarahcustomRLEnv, self).__init__()\n",
    "        self.process_data = process_data\n",
    "        self.process_index = 0\n",
    "        self.current_time = 0\n",
    "\n",
    "        self.action_space = spaces.Discrete(len(self.process_data))\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        selected_process = self.process_data.iloc[action]\n",
    "        burst_time = selected_process['Burst time']\n",
    "        self.current_time += burst_time\n",
    "        reward = -burst_time\n",
    "\n",
    "        self.process_index += 1\n",
    "        done = self.process_index >= len(self.process_data)\n",
    "        next_state = (\n",
    "            np.array([self.current_time, self.process_data.iloc[self.process_index]['Burst time'], self.process_data.iloc[self.process_index]['Resources']])\n",
    "            if not done else np.array([0, 0, 0])\n",
    "        )\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_time = 0\n",
    "        self.process_index = 0\n",
    "        initial_process = self.process_data.iloc[self.process_index]\n",
    "        return np.array([self.current_time, initial_process['Burst time'], initial_process['Resources']])\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "env = SarahcustomRLEnv(process_data=df)\n",
    "\n",
    "def optimize_ppo(trial):\n",
    "    # Define the hyperparameters search space\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
    "    n_steps = trial.suggest_categorical('n_steps', [2048, 4096, 8192])\n",
    "    \n",
    "    # Vectorize the environment for parallel training\n",
    "    vec_env = make_vec_env(lambda: SarahcustomRLEnv(process_data=df), n_envs=4)\n",
    "\n",
    "    # Create the PPO model\n",
    "    model = PPO('MlpPolicy', vec_env, verbose=0, learning_rate=learning_rate, batch_size=batch_size, gamma=gamma, n_steps=n_steps)\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Evaluate the model\n",
    "    rewards = []\n",
    "    for _ in range(10):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# Step 3: Create Optuna Study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_ppo, n_trials=50)\n",
    "\n",
    "# Step 4: Print the Best Hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "vec_env = make_vec_env(lambda: SarahcustomRLEnv(process_data=df), n_envs=4)\n",
    "model = PPO('MlpPolicy', vec_env, verbose=1, **best_params)\n",
    "\n",
    "model.learn(total_timesteps=10000)\n",
    "model.save(\"Sarah_custom_model\")\n",
    "\n",
    "# Load the model (if needed)\n",
    "model = PPO.load(\"Sarah_custom_model\")\n",
    "\n",
    "# Evaluate the Trained Agent\n",
    "evaluation_metrics = {\n",
    "    'total_rewards': [],\n",
    "    'average_burst_time': [],\n",
    "    'average_waiting_time': [],\n",
    "    'average_turnaround_time': [],\n",
    "    'resource_utilization': [],\n",
    "    'throughput': []\n",
    "}\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    process_times = []\n",
    "    waiting_times = []\n",
    "    turnaround_times = []\n",
    "    resource_usage = 0\n",
    "    start_time = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        process_times.append(env.process_data.iloc[action]['Burst time'])\n",
    "        waiting_times.append(env.current_time - env.process_data.iloc[action]['Arrival Time'])\n",
    "        turnaround_times.append(env.current_time - env.process_data.iloc[action]['Arrival Time'] + env.process_data.iloc[action]['Burst time'])\n",
    "        resource_usage += env.process_data.iloc[action]['Resources']\n",
    "\n",
    "    evaluation_metrics['total_rewards'].append(total_reward)\n",
    "    evaluation_metrics['average_burst_time'].append(np.mean(process_times))\n",
    "    evaluation_metrics['average_waiting_time'].append(np.mean(waiting_times))\n",
    "    evaluation_metrics['average_turnaround_time'].append(np.mean(turnaround_times))\n",
    "    evaluation_metrics['resource_utilization'].append(resource_usage / len(process_times))\n",
    "    evaluation_metrics['throughput'].append(len(process_times) / (env.current_time - start_time))\n",
    "\n",
    "# Print the averaged metrics\n",
    "for metric, values in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {np.mean(values)}\")\n",
    "\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
